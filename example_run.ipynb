{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7dbea4",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaec8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import time\n",
    "from griffin_lim_algs import naive_griffin_lim, fast_griffin_lim, accelerated_griffin_lim\n",
    "# from deep_agla import DeepAGLA\n",
    "from deep_agla import DeepAGLA\n",
    "from eval_metrics import evaluate_batch\n",
    "from definitions import *\n",
    "\n",
    "device = \"cuda\"\n",
    "N_ITER = 64\n",
    "times_algos = {}\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8fad77",
   "metadata": {},
   "source": [
    "# Compare Model Vs. Original Algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becbcfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(DATA_PATH)\n",
    "print(f\"Data Shape: {data.shape}\")\n",
    "test_sample = data[0]\n",
    "print(f\"Sample Shape: {test_sample.shape}\")\n",
    "Audio(test_sample, rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c0888",
   "metadata": {},
   "source": [
    "## Quick Training Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick training test with small subset of data\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from dataset import AudioDataset\n",
    "import os\n",
    "\n",
    "# Create a small subset of the data for quick training\n",
    "print(\"Setting up quick training...\")\n",
    "\n",
    "# Use only first 32 samples for quick training\n",
    "subset_size = 32\n",
    "full_dataset = AudioDataset(npy_path=DATA_PATH)\n",
    "subset_indices = list(range(min(subset_size, len(full_dataset))))\n",
    "train_subset = Subset(full_dataset, subset_indices)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_subset, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "# Initialize model for training\n",
    "model = DeepAGLA(\n",
    "    n_layers=64,\n",
    "    lr=1e-3,\n",
    "    loss_weights={\n",
    "        'time_l1': 0.9,\n",
    "        'time_mse': 0.9, \n",
    "        'spec_l1': 0.1,\n",
    "        'log_spec_l1': 0.1\n",
    "    }\n",
    ")\n",
    "\n",
    "# Setup trainer for quick training\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    logger=False,  # Disable logging for quick test\n",
    "    enable_checkpointing=True,\n",
    "    default_root_dir='./quick_training_test',\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Starting quick training with {len(train_subset)} samples for 10 epochs...\")\n",
    "trainer.fit(model, train_loader)\n",
    "\n",
    "# Save the trained model\n",
    "save_path = './quick_trained_model.ckpt'\n",
    "trainer.save_checkpoint(save_path)\n",
    "print(f\"Quick training completed! Model saved to: {save_path}\")\n",
    "print(f\"Final training loss: {trainer.callback_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea615a41",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91df65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = save_path\n",
    "model = DeepAGLA.load_from_checkpoint(\n",
    "    CHECKPOINT_PATH,\n",
    "    map_location=device\n",
    ")\n",
    "model.set_inference_mode(use_all_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = DeepAGLA(\n",
    "    n_layers=64,\n",
    "    lr=1e-3,\n",
    "    loss_weights={\n",
    "        'time_l1': 0.9,\n",
    "        'time_mse': 0.9, \n",
    "        'spec_l1': 0.1,\n",
    "        'log_spec_l1': 0.1\n",
    "    })\n",
    "\n",
    "# Compute the diff in alpha/beta/gamma values for each layer, then plot the diff for each parameter over layers\n",
    "alpha_diffs = []\n",
    "beta_diffs = []\n",
    "gamma_diffs = []\n",
    "for i in range(model.n_layers):\n",
    "    layer_trained = model.layers[i]\n",
    "    layer_test = model_test.layers[i]\n",
    "    alpha_diffs.append(layer_trained.alpha.item() - layer_test.alpha.item())\n",
    "    beta_diffs.append(layer_trained.beta.item() - layer_test.beta.item())\n",
    "    gamma_diffs.append(layer_trained.gamma.item() - layer_test.gamma.item())\n",
    "    \n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(model.n_layers), alpha_diffs, marker='o', label='Alpha Diff')\n",
    "plt.title('Alpha Differences Across Layers')\n",
    "plt.xlabel('Layer Index')\n",
    "plt.ylabel('Alpha Diff')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(model.n_layers), beta_diffs, marker='o', label='Beta Diff')\n",
    "plt.title('Beta Differences Across Layers')\n",
    "plt.xlabel('Layer Index')\n",
    "plt.ylabel('Beta Diff')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(model.n_layers), gamma_diffs, marker='o', label='Gamma Diff')\n",
    "plt.title('Gamma Differences Across Layers')\n",
    "plt.xlabel('Layer Index')\n",
    "plt.ylabel('Gamma Diff')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "016dbe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_model = time.time()\n",
    "recon_model, losses = model.inference_forward(torch.tensor(test_sample).to(device).unsqueeze(0))\n",
    "end_time_model = time.time()\n",
    "times_algos['Model'] = end_time_model - start_time_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef0b6f",
   "metadata": {},
   "source": [
    "## Test Original GLAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87bb072",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_naive = time.time()\n",
    "recon_naive, metrics_naive, losses_naive = naive_griffin_lim(test_sample, n_iter=N_ITER)\n",
    "end_time_naive = time.time()\n",
    "times_algos['Naive'] = end_time_naive - start_time_naive\n",
    "\n",
    "start_time_fast = time.time()\n",
    "recon_fast, metrics_fast, losses_fast = fast_griffin_lim(test_sample, n_iter=N_ITER)\n",
    "end_time_fast = time.time()\n",
    "times_algos['Fast'] = end_time_fast - start_time_fast\n",
    "start_time_accel = time.time()\n",
    "recon_accel, metrics_accel, losses_accel = accelerated_griffin_lim(test_sample, n_iter=N_ITER)\n",
    "end_time_accel = time.time()\n",
    "times_algos['Accelerated'] = end_time_accel - start_time_accel\n",
    "\n",
    "# Match the model losses dict to the Griffin-Lim losses, currently has different keys and no iteration key\n",
    "# There is a total key (not needed), and each value is a list over iterations\n",
    "model_losses_matched = []\n",
    "for iter in range(N_ITER):\n",
    "    model_losses_matched.append({\n",
    "        'Iteration': iter+1,\n",
    "        'L1 Waveform': losses['time_l1'][iter],\n",
    "        'MSE Waveform': losses['time_mse'][iter],\n",
    "        'L1 Spectral': losses['spec_l1'][iter],\n",
    "        'Log L1 Spectral': losses['log_spec_l1'][iter],\n",
    "        'Total Loss': losses['total'][iter]\n",
    "    })\n",
    "\n",
    "# Plot each loss type over iterations for each algorithm\n",
    "# losses_<algo> is a list of dictionaries with keys 'Iteration', 'L1 Waveform', 'MSE Waveform', 'L1 Spectral', 'Log L1 Spectral'\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot([loss['Iteration'] for loss in losses_naive], [loss['L1 Waveform'] for loss in losses_naive], label='Naive L1 Waveform')\n",
    "plt.plot([loss['Iteration'] for loss in losses_fast], [loss['L1 Waveform'] for loss in losses_fast], label='Fast L1 Waveform')\n",
    "plt.plot([loss['Iteration'] for loss in losses_accel], [loss['L1 Waveform'] for loss in losses_accel], label='Accel L1 Waveform')\n",
    "plt.plot([loss['Iteration'] for loss in model_losses_matched], [loss['L1 Waveform'] for loss in model_losses_matched], label='Model L1 Waveform', linestyle='--')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('L1 Waveform Loss')\n",
    "plt.title('L1 Waveform Loss Over Iterations')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot([loss['Iteration'] for loss in losses_naive], [loss['MSE Waveform'] for loss in losses_naive], label='Naive MSE Waveform')\n",
    "plt.plot([loss['Iteration'] for loss in losses_fast], [loss['MSE Waveform'] for loss in losses_fast], label='Fast MSE Waveform')\n",
    "plt.plot([loss['Iteration'] for loss in losses_accel], [loss['MSE Waveform'] for loss in losses_accel], label='Accel MSE Waveform')\n",
    "plt.plot([loss['Iteration'] for loss in model_losses_matched], [loss['MSE Waveform'] for loss in model_losses_matched], label='Model MSE Waveform', linestyle='--')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Waveform Loss')\n",
    "plt.title('MSE Waveform Loss Over Iterations')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot([loss['Iteration'] for loss in losses_naive], [loss['L1 Spectral'] for loss in losses_naive], label='Naive L1 Spectral')\n",
    "plt.plot([loss['Iteration'] for loss in losses_fast], [loss['L1 Spectral'] for loss in losses_fast], label='Fast L1 Spectral')\n",
    "plt.plot([loss['Iteration'] for loss in losses_accel], [loss['L1 Spectral'] for loss in losses_accel], label='Accel L1 Spectral')\n",
    "plt.plot([loss['Iteration'] for loss in model_losses_matched], [loss['L1 Spectral'] for loss in model_losses_matched], label='Model L1 Spectral', linestyle='--')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('L1 Spectral Loss')\n",
    "plt.title('L1 Spectral Loss Over Iterations')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot([loss['Iteration'] for loss in losses_naive], [loss['Log L1 Spectral'] for loss in losses_naive], label='Naive Log L1 Spectral')\n",
    "plt.plot([loss['Iteration'] for loss in losses_fast], [loss['Log L1 Spectral'] for loss in losses_fast], label='Fast Log L1 Spectral')\n",
    "plt.plot([loss['Iteration'] for loss in losses_accel], [loss['Log L1 Spectral'] for loss in losses_accel], label='Accel Log L1 Spectral')\n",
    "plt.plot([loss['Iteration'] for loss in model_losses_matched], [loss['Log L1 Spectral'] for loss in model_losses_matched], label='Model Log L1 Spectral', linestyle='--')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log L1 Spectral Loss')\n",
    "plt.title('Log L1 Spectral Loss Over Iterations')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Total Loss Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot([loss['Iteration'] for loss in losses_naive], [loss['Total Loss'] for loss in losses_naive], label='Naive Total Loss')\n",
    "plt.plot([loss['Iteration'] for loss in losses_fast], [loss['Total Loss'] for loss in losses_fast], label='Fast Total Loss')\n",
    "plt.plot([loss['Iteration'] for loss in losses_accel], [loss['Total Loss'] for loss in losses_accel], label='Accel Total Loss')\n",
    "plt.plot([loss['Iteration'] for loss in model_losses_matched], [loss['Total Loss'] for loss in model_losses_matched], label='Model Total Loss', linestyle='--')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Total Loss Over Iterations')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot of times for each algorithm\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(times_algos.keys(), times_algos.values(), color=['blue', 'orange', 'green'])\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.title('Time Taken by Each Griffin-Lim Algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3087c",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8441225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Evaluation (the same ones that came back from the algorithms, done on the model output)\n",
    "\n",
    "from griffin_lim_algs import compute_all_metrics, match_signals\n",
    "\n",
    "# Properly align the signals before computing metrics\n",
    "model_recon = recon_model.squeeze().cpu().numpy()\n",
    "aligned_original, aligned_model_recon = match_signals(test_sample, model_recon)\n",
    "model_metrics = compute_all_metrics(aligned_original, aligned_model_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced bar plots for the metrics comparing the algorithms and model\n",
    "\n",
    "metrics = model_metrics.keys()\n",
    "\n",
    "# Define which metrics are \"higher is better\" vs \"lower is better\"\n",
    "higher_is_better = {'SNR (dB)', 'SSNR (dB)', 'PSNR (dB)', 'SER (dB)', 'SISDR (dB)', 'SISNR (dB)', 'STOI'}\n",
    "lower_is_better = {'MSE', 'MAE', 'LSD (dB)', 'Spectral Convergence', 'THD (%)'}\n",
    "\n",
    "algorithm_names = list(times_algos.keys())\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Get values for all algorithms\n",
    "    values = [model_metrics[metric], metrics_naive[metric][-1], metrics_fast[metric][-1], metrics_accel[metric][-1]]\n",
    "    \n",
    "    # Find the best performing algorithm\n",
    "    if metric in higher_is_better:\n",
    "        best_idx = np.argmax(values)\n",
    "    elif metric in lower_is_better:\n",
    "        best_idx = np.argmin(values)\n",
    "    else:\n",
    "        # Default to higher is better for unknown metrics\n",
    "        best_idx = np.argmax(values)\n",
    "    \n",
    "    # Create edge colors - highlight the best with yellow outline, others with black\n",
    "    edge_colors = ['black'] * len(values)\n",
    "    edge_colors[best_idx] = 'gold'\n",
    "    edge_widths = [1] * len(values)\n",
    "    edge_widths[best_idx] = 3\n",
    "    \n",
    "    # Create the bar plot\n",
    "    bars = plt.bar(algorithm_names, values, color=colors, edgecolor=edge_colors, linewidth=edge_widths)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for i, (bar, value) in enumerate(zip(bars, values)):\n",
    "        height = bar.get_height()\n",
    "        # Format the value based on the metric type\n",
    "        if 'dB' in metric:\n",
    "            value_str = f'{value:.2f}'\n",
    "        elif metric in ['MSE', 'MAE', 'Spectral Convergence']:\n",
    "            value_str = f'{value:.4f}'\n",
    "        elif metric == 'STOI':\n",
    "            value_str = f'{value:.3f}'\n",
    "        elif metric == 'THD (%)':\n",
    "            value_str = f'{value:.2f}%'\n",
    "        else:\n",
    "            value_str = f'{value:.2f}'\n",
    "        \n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + abs(height)*0.01,\n",
    "                value_str, ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.xlabel('Algorithm')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f'{metric} Comparison')\n",
    "    \n",
    "    # Add a subtle grid\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Adjust y-axis to accommodate the value labels\n",
    "    y_min, y_max = plt.ylim()\n",
    "    y_range = y_max - y_min\n",
    "    plt.ylim(y_min, y_max + y_range * 0.1)\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
